{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# logging for gensim (set to INFO)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Dataframe Import\n",
    "#Open Corpus of News Article Text\n",
    "with open('./data/credible_news_df.pickle', 'rb') as file:\n",
    "     credible_news_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in fake news data to pandas dataframe\n",
    "not_credible_news_df = pd.read_csv('./data/Not_Credible/fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to preprocessing\n",
    "\n",
    "1- Remove capitalization and punctuation\n",
    "\n",
    "2- Remove overfit words/phrases (including source names, format-specific words (e.g., one source listed the day of the week in the first line of every article), and phrases contained in every article â€“usually a header/footer); \n",
    "\n",
    "3- Remove short words (words less than 3 characters long); \n",
    "\n",
    "4- Remove stop words.\n",
    "\n",
    "5- Convert numbers into words or removing numbers\n",
    "\n",
    "6- expanding abbreviations\n",
    "\n",
    "7- text canoncalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Real News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numbers, captial letters and punctuation from article text and title\n",
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "credible_news_df['text'] = credible_news_df.text.map(alphanumeric).map(punc_lower)\n",
    "credible_news_df['title'] = credible_news_df.title.map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from text\n",
    "credible_news_df['text'] = credible_news_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "credible_news_df['title'] = credible_news_df['title'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove All Spaces\n",
    "credible_news_df['text'] = credible_news_df['text'].apply(lambda x: ' '.join(x.split()))\n",
    "credible_news_df['title'] = credible_news_df['title'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_news_df['text']= credible_news_df['text'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_news_df['title']= credible_news_df['title'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Target Column \n",
    "# Column Value = 1 Means Article is not Real\n",
    "# Column Value = 0 Means Article is Real\n",
    "credible_news_df['Not_Real_or_Real'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save Updated Data Frame\n",
    "with open('./data/credible_news_df_cleaned.pickle', 'wb') as file:\n",
    "     pickle.dump(credible_news_df, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Fake News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose subset of fake news that is 1.5 size of real news\n",
    "not_credible_news_df = not_credible_news_df.loc[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all Non Value Rows from both text and titles\n",
    "not_credible_news_df.dropna(inplace=True)\n",
    "\n",
    "#Reshape Matrix Indices\n",
    "not_credible_news_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numbers, captial letters and punctuation from article text and title\n",
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "not_credible_news_df['text'] = not_credible_news_df.text.map(alphanumeric).map(punc_lower)\n",
    "not_credible_news_df['title'] = not_credible_news_df.title.map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from text\n",
    "not_credible_news_df['text'] = not_credible_news_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "not_credible_news_df['title'] = not_credible_news_df['title'].apply(lambda x: re.sub(r'[^\\w\\s]','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove All Spaces\n",
    "not_credible_news_df['text'] = not_credible_news_df['text'].apply(lambda x: ' '.join(x.split()))\n",
    "not_credible_news_df['title'] = not_credible_news_df['title'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_credible_news_df['text']= not_credible_news_df['text'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_credible_news_df['title']= not_credible_news_df['title'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_credible_news_df.rename(index=str, columns={\"index\": \"label\"} ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_credible_news_df.label=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Target Column \n",
    "# Column Value = 1 Means Article is not Real\n",
    "# Column Value = 0 Means Article is Real\n",
    "not_credible_news_df['Not_Real_or_Real'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save Updated Data Frame\n",
    "with open('./data/not_credible_news_df_cleaned.pickle', 'wb') as file:\n",
    "     pickle.dump(not_credible_news_df, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Reduced News Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge both dataframes\n",
    "news_data_frame = pd.concat([credible_news_df, not_credible_news_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_frame = news_data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not_Real_or_Real</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the ban united states companies selling parts ...</td>\n",
       "      <td>huawei blacklisted starting friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>washington hatice cengiz the fiancee murdered ...</td>\n",
       "      <td>world has done nothing khashoggi fiancee gives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>least six civilians including women and childr...</td>\n",
       "      <td>saudi uae coalition carries out deadly air rai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Not_Real_or_Real label                                               text  \\\n",
       "0                 0     0  the ban united states companies selling parts ...   \n",
       "1                 0     0  washington hatice cengiz the fiancee murdered ...   \n",
       "2                 0     0  least six civilians including women and childr...   \n",
       "\n",
       "                                               title  \n",
       "0                 huawei blacklisted starting friday  \n",
       "1  world has done nothing khashoggi fiancee gives...  \n",
       "2  saudi uae coalition carries out deadly air rai...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_frame.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not_Real_or_Real</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dear president how many jobs will building wal...</td>\n",
       "      <td>dear president how many jobs will building wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>why the trump reflation trade for chumps david...</td>\n",
       "      <td>why the trump reflation trade for chumps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>chart the day the trump rally all dow mind the...</td>\n",
       "      <td>chart the day the trump rally all dowmind the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Not_Real_or_Real label  \\\n",
       "4423                 1     0   \n",
       "4424                 1     0   \n",
       "4425                 1     0   \n",
       "\n",
       "                                                   text  \\\n",
       "4423  dear president how many jobs will building wal...   \n",
       "4424  why the trump reflation trade for chumps david...   \n",
       "4425  chart the day the trump rally all dow mind the...   \n",
       "\n",
       "                                                  title  \n",
       "4423  dear president how many jobs will building wal...  \n",
       "4424           why the trump reflation trade for chumps  \n",
       "4425  chart the day the trump rally all dowmind the ...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_frame.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save Updated Data Frame\n",
    "with open('./data/news_fake_real_df_reduced.pickle', 'wb') as file:\n",
    "     pickle.dump(news_data_frame, file)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "`process of splitting the given text into smaller pieces called tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words and tokeniz individual texts\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "news_data_frame['tokenized_text'] = news_data_frame['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_frame['tokenized_title'] = news_data_frame['title'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "news_data_frame['tokenized_text']= news_data_frame['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "news_data_frame['tokenized_title']= news_data_frame['tokenized_title'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Save Updated Data Frame\n",
    "# with open('./data/news_fake_real_df_reduced_token.pickle', 'wb') as file:\n",
    "#       pickle.dump(news_data_frame, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfid Vectorization\n",
    "- Gives the relative importance of a term in a corpus (text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid_vectorization function\n",
    "def tfid_vectorization(df, column_to_vectorize=None, vectorized_name=None):\n",
    "    # list of text documents\n",
    "    article= df[column_to_vectorize]\n",
    "\n",
    "    # create the transform\n",
    "    vectorizer= TfidfVectorizer()\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(article)\n",
    "\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    #print(vectorizer.idf_)\n",
    "\n",
    "    # # # encode documents\n",
    "    df[vectorized_name] = article.apply(lambda x: vectorizer.transform([x]))\n",
    "    \n",
    "    # # summarize encoded vector\n",
    "    #print(df[vectorized_name].shape)\n",
    "    #print(df[vectorized_name].toarray())\n",
    "    \n",
    "    print('Tfid Vectorization Completed \\n')\n",
    "    \n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfid Vectorization Completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tfid vectorization of document text\n",
    "tfid_vectorization(news_data_frame,'text','tfid_vec_text');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfid Vectorization Completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tfid vectorization of document titles\n",
    "tfid_vectorization(news_data_frame,'title','tfid_vec_title');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "- Convert all disparities of a word into their normalized form as a part of feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using NLTK\n",
    "-Stemming is a process of linguistic normalization, which reduces words to their root word or chops off the derviational affices. For example, connection, connected, connecting word reduce to a common word 'connect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Stemming Text\n",
    "news_data_frame['stemmed_text'] = news_data_frame['tokenized_text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "#Stemming Titles\n",
    "news_data_frame['stemmed_title'] = news_data_frame['tokenized_title'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Lemmatizing Text\n",
    "news_data_frame['lemmatized_text'] = news_data_frame['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "\n",
    "#Lemmatizing Titles\n",
    "news_data_frame['lemmatized_title'] = news_data_frame['tokenized_title'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS and Chunking Text\n",
    "- Helps overcome bagofwords weakness which fails to capture the structure of sentences and sometimes gives its appropriate meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "#POS Text\n",
    "news_data_frame['pos_text'] = news_data_frame['tokenized_text'].apply(lambda x: [nltk.pos_tag([y]) for y in x])\n",
    "\n",
    "#POS Titles\n",
    "news_data_frame['pos_title'] = news_data_frame['tokenized_title'].apply(lambda x: [nltk.pos_tag([y]) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_frame.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save Updated Data Frame\n",
    "with open('./data/news_data_frame_reduced_preprocessed.pickle', 'wb') as file:\n",
    "     pickle.dump(news_data_frame, file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
